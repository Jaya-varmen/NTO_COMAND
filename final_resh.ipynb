{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c530f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "W_READ = 0.7\n",
    "W_ANY  = 0.3\n",
    "\n",
    "W_XGB = 0.5\n",
    "W_CB  = 0.5\n",
    "\n",
    "N_COLD  = 15   \n",
    "SPLIT_Q = 0.8  \n",
    "\n",
    "DATA_DIR = \"/Users/steksov_grigoriy/Desktop/НТО/individ/public\" #  указать путь\n",
    "\n",
    "TRAIN_PATH       = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TARGETS_PATH     = os.path.join(DATA_DIR, \"targets.csv\")\n",
    "CANDIDATES_PATH  = os.path.join(DATA_DIR, \"candidates.csv\")\n",
    "BOOKS_PATH       = os.path.join(DATA_DIR, \"books.csv\")\n",
    "GENRES_PATH      = os.path.join(DATA_DIR, \"genres.csv\")\n",
    "BOOK_GENRES_PATH = os.path.join(DATA_DIR, \"book_genres.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "266ac2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (269061, 5)\n",
      "targets (3512, 1)\n",
      "candidates (3512, 2)\n",
      "   user_id  book_id  has_read  rating            timestamp\n",
      "0     3870   310170         0       0  2008-04-27 21:06:16\n",
      "1     3870   306406         0       0  2008-06-07 11:51:01\n",
      "2     4091   195676         0       0  2008-08-06 00:40:55\n",
      "3     3870   554261         1       8  2008-08-07 09:16:12\n",
      "4     3870    33078         1       2  2008-08-07 09:17:20\n",
      "   user_id                                       book_id_list\n",
      "0      210  11936,254097,709075,840500,971259,1037723,1074...\n",
      "1     1380  8369,28302,145975,482934,625734,998313,1098150...\n",
      "2     2050  4902,8369,18790,308364,317849,460492,822326,86...\n",
      "3     2740  39221,112023,149611,162418,181062,317050,43565...\n",
      "4     4621  28638,28639,28642,28901,31479,307058,475353,57...\n",
      "books shape (55785, 8)\n",
      "genres shape (439, 3)\n",
      "book genres shape (103646, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "targets = pd.read_csv(TARGETS_PATH)\n",
    "candidates_raw = pd.read_csv(CANDIDATES_PATH)\n",
    "\n",
    "print(\"train\", train.shape)\n",
    "print(\"targets\", targets.shape)\n",
    "print(\"candidates\", candidates_raw.shape)\n",
    "print(train.head())\n",
    "print(candidates_raw.head())\n",
    "\n",
    "if os.path.exists(BOOKS_PATH):\n",
    "    books = pd.read_csv(BOOKS_PATH)\n",
    "    print(\"books shape\", books.shape)\n",
    "else:\n",
    "    books = None\n",
    "    print(\"error 1\")\n",
    "\n",
    "if os.path.exists(GENRES_PATH) and os.path.exists(BOOK_GENRES_PATH):\n",
    "    genres = pd.read_csv(GENRES_PATH)\n",
    "    book_genres = pd.read_csv(BOOK_GENRES_PATH)\n",
    "    print(\"genres shape\", genres.shape)\n",
    "    print(\"book genres shape\", book_genres.shape)\n",
    "else:\n",
    "    genres = None\n",
    "    book_genres = None\n",
    "    print(\"error 2\")\n",
    "\n",
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678cda01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T split 2020-09-11 23:28:35\n",
      "train hist shape (215249, 5)\n",
      "val period shape (53812, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_point = train[\"timestamp\"].quantile(SPLIT_Q)\n",
    "print(\"T split\", split_point)\n",
    "\n",
    "train_hist = train[train[\"timestamp\"] <= split_point].copy()\n",
    "val_period = train[train[\"timestamp\"] > split_point].copy()\n",
    "\n",
    "print(\"train hist shape\", train_hist.shape)\n",
    "print(\"val period shape\", val_period.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_history_and_popularity(df: pd.DataFrame):\n",
    "    user_hist_books = (\n",
    "        df.groupby(\"user_id\")[\"book_id\"]\n",
    "          .agg(lambda x: set(x.tolist()))\n",
    "          .to_dict()\n",
    "    )\n",
    "    book_pop = (\n",
    "        df.groupby(\"book_id\")[\"user_id\"]\n",
    "          .nunique()\n",
    "          .sort_values(ascending=False)\n",
    "    )\n",
    "    popular_books = book_pop.index.to_numpy()\n",
    "    return user_hist_books, popular_books\n",
    "\n",
    "\n",
    "def sample_cold_candidates_for_user(user_id, user_hist_books, popular_books, n_cold=15):\n",
    "    seen = user_hist_books.get(user_id, set())\n",
    "    cold = [b for b in popular_books if b not in seen]\n",
    "    if len(cold) > n_cold:\n",
    "        cold = cold[:n_cold]\n",
    "    return cold\n",
    "\n",
    "\n",
    "def build_cold_candidates(users, user_hist_books, popular_books, n_cold=15):\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        for b in sample_cold_candidates_for_user(u, user_hist_books, popular_books, n_cold=n_cold):\n",
    "            rows.append((u, b, 0)) \n",
    "    return pd.DataFrame(rows, columns=[\"user_id\", \"book_id\", \"rel\"])\n",
    "\n",
    "\n",
    "def dcg_at_k(rels, k=20):\n",
    "    rels = np.asarray(rels)[:k]\n",
    "    if rels.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.sum(rels / np.log2(np.arange(2, len(rels) + 2))))\n",
    "\n",
    "\n",
    "def ndcg_for_user(df_u, k=20):\n",
    "    df_sorted = df_u.sort_values(\"pred\", ascending=False)\n",
    "    rels_pred = df_sorted[\"rel\"].values\n",
    "    dcg = dcg_at_k(rels_pred, k=k)\n",
    "    ideal_rels = np.sort(df_u[\"rel\"].values)[::-1]\n",
    "    idcg = dcg_at_k(ideal_rels, k=k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def mean_ndcg(df_all, k=20):\n",
    "    scores = []\n",
    "    for _, df_u in df_all.groupby(\"user_id\"):\n",
    "        scores.append(ndcg_for_user(df_u, k=k))\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "\n",
    "def make_submission_user_list(df_pred: pd.DataFrame, top_k: int = 20) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for uid, df_u in df_pred.groupby(\"user_id\"):\n",
    "        df_sorted = df_u.sort_values(\"pred\", ascending=False)\n",
    "        df_sorted = df_sorted.drop_duplicates(subset=\"book_id\", keep=\"first\")\n",
    "        top_books = df_sorted[\"book_id\"].head(top_k).tolist()\n",
    "        rows.append((uid, \",\".join(map(str, top_books))))\n",
    "    return pd.DataFrame(rows, columns=[\"user_id\", \"book_id_list\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5455248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  u_n_books  u_n_events  u_n_read  u_n_plan  u_read_share\n",
      "0      151         75          75        36        39      0.480000\n",
      "1      210         31          31         0        31      0.000000\n",
      "2      560          5           5         0         5      0.000000\n",
      "3     1380         46          46        19        27      0.413043\n",
      "4     1850         77          77        38        39      0.493506\n",
      "   book_id  b_n_users  b_n_events  b_n_read  b_n_plan  b_read_rate\n",
      "0       20        111         111        94        17     0.846847\n",
      "1       35          1           1         1         0     0.999999\n",
      "2       52          1           1         1         0     0.999999\n",
      "3       54          5           5         4         1     0.800000\n",
      "4       69          1           1         1         0     0.999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_user_book_stats(df: pd.DataFrame):\n",
    "    book_stats = (\n",
    "        df.groupby(\"book_id\")\n",
    "          .agg(\n",
    "              b_n_users=(\"user_id\", \"nunique\"),\n",
    "              b_n_events=(\"user_id\", \"size\"),\n",
    "              b_n_read=(\"has_read\", lambda x: int((x == 1).sum())),\n",
    "              b_n_plan=(\"has_read\", lambda x: int((x == 0).sum())),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "    denom = book_stats[\"b_n_read\"] + book_stats[\"b_n_plan\"] + 1e-6\n",
    "    book_stats[\"b_read_rate\"] = book_stats[\"b_n_read\"] / denom\n",
    "\n",
    "    user_stats = (\n",
    "        df.groupby(\"user_id\")\n",
    "          .agg(\n",
    "              u_n_books=(\"book_id\", \"nunique\"),\n",
    "              u_n_events=(\"book_id\", \"size\"),\n",
    "              u_n_read=(\"has_read\", lambda x: int((x == 1).sum())),\n",
    "              u_n_plan=(\"has_read\", lambda x: int((x == 0).sum())),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "    user_stats[\"u_read_share\"] = user_stats[\"u_n_read\"] / (user_stats[\"u_n_events\"] + 1e-6)\n",
    "    return user_stats, book_stats\n",
    "\n",
    "\n",
    "user_stats_hist, book_stats_hist = compute_user_book_stats(train_hist)\n",
    "print(user_stats_hist.head())\n",
    "print(book_stats_hist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d249d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_interaction_features(candidates: pd.DataFrame, hist_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = candidates.copy()\n",
    "\n",
    "    pair_hist = hist_df[[\"user_id\", \"book_id\"]].drop_duplicates()\n",
    "    pair_hist[\"has_interacted\"] = 1\n",
    "    df = df.merge(pair_hist, on=[\"user_id\", \"book_id\"], how=\"left\")\n",
    "    df[\"has_interacted\"] = df[\"has_interacted\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "    if \"rating\" in hist_df.columns:\n",
    "        rating_map = (\n",
    "            hist_df.dropna(subset=[\"rating\"])\n",
    "                   .groupby([\"user_id\", \"book_id\"])[\"rating\"]\n",
    "                   .mean()\n",
    "                   .reset_index()\n",
    "                   .rename(columns={\"rating\": \"user_book_rating\"})\n",
    "        )\n",
    "        df = df.merge(rating_map, on=[\"user_id\", \"book_id\"], how=\"left\")\n",
    "        df[\"user_book_rating\"] = df[\"user_book_rating\"].fillna(0).astype(\"float32\")\n",
    "    else:\n",
    "        df[\"user_book_rating\"] = 0.0\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb2a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(candidates: pd.DataFrame,\n",
    "                          hist_df: pd.DataFrame,\n",
    "                          split_time=None) -> pd.DataFrame:\n",
    "    df = candidates.copy()\n",
    "    logs = hist_df.copy()\n",
    "\n",
    "    user_read_ts = (\n",
    "        logs[logs[\"has_read\"] == 1]\n",
    "        .groupby(\"user_id\")[\"timestamp\"]\n",
    "        .agg([\"mean\", \"max\", \"min\"])\n",
    "        .add_prefix(\"u_read_ts_\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    book_ts = (\n",
    "        logs.groupby(\"book_id\")[\"timestamp\"]\n",
    "            .agg([\"mean\", \"max\", \"min\"])\n",
    "            .add_prefix(\"b_ts_\")\n",
    "            .reset_index()\n",
    "    )\n",
    "\n",
    "    df = df.merge(user_read_ts, on=\"user_id\", how=\"left\")\n",
    "    df = df.merge(book_ts, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    last_ts = logs[\"timestamp\"].max()\n",
    "    df[\"days_since_book_event\"] = (\n",
    "        (last_ts - df[\"b_ts_max\"]).dt.total_seconds() / (24 * 3600)\n",
    "    )\n",
    "    df[\"days_since_book_event\"] = df[\"days_since_book_event\"].fillna(1e4).astype(\"float32\")\n",
    "\n",
    "    logs[\"year\"] = logs[\"timestamp\"].dt.year\n",
    "    last_year = logs[\"year\"].max()\n",
    "    pop_last_year = (\n",
    "        logs[logs[\"year\"] == last_year]\n",
    "        .groupby(\"book_id\")[\"user_id\"]\n",
    "        .nunique()\n",
    "        .rename(\"book_popularity_last_year\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(pop_last_year, on=\"book_id\", how=\"left\")\n",
    "    df[\"book_popularity_last_year\"] = df[\"book_popularity_last_year\"].fillna(0).astype(\"float32\")\n",
    "\n",
    "    time_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n",
    "    if time_cols:\n",
    "        print(\"Удаляю datetime-фичи:\", time_cols)\n",
    "        df = df.drop(columns=time_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_popularity_trend_features(candidates: pd.DataFrame,\n",
    "                                  hist_df: pd.DataFrame,\n",
    "                                  window_days: int = 180) -> pd.DataFrame:\n",
    "    df = candidates.copy()\n",
    "    logs = hist_df.copy()\n",
    "\n",
    "    logs[\"ts_days\"] = logs[\"timestamp\"].view(\"int64\") // (24 * 3600 * 10**9)\n",
    "    last_day = logs[\"ts_days\"].max()\n",
    "\n",
    "    recent_start = last_day - window_days\n",
    "    prev_start = last_day - 2 * window_days\n",
    "    prev_end = recent_start\n",
    "\n",
    "    recent = (\n",
    "        logs[logs[\"ts_days\"] >= recent_start]\n",
    "        .groupby(\"book_id\")[\"user_id\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"user_id\": \"b_users_recent\"})\n",
    "    )\n",
    "    prev = (\n",
    "        logs[(logs[\"ts_days\"] >= prev_start) & (logs[\"ts_days\"] < prev_end)]\n",
    "        .groupby(\"book_id\")[\"user_id\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"user_id\": \"b_users_prev\"})\n",
    "    )\n",
    "\n",
    "    trend = recent.merge(prev, on=\"book_id\", how=\"outer\").fillna(0)\n",
    "    trend[\"b_pop_ratio\"] = (trend[\"b_users_recent\"] + 1.0) / (trend[\"b_users_prev\"] + 1.0)\n",
    "    trend[\"b_pop_diff\"] = trend[\"b_users_recent\"] - trend[\"b_users_prev\"]\n",
    "\n",
    "    df = df.merge(trend, on=\"book_id\", how=\"left\").fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c6fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_books_metadata(books_df, genres_df, book_genres_df):\n",
    "    if books_df is None or genres_df is None or book_genres_df is None:\n",
    "        return None\n",
    "\n",
    "    g = genres_df.copy()\n",
    "    if \"genre\" not in g.columns:\n",
    "        for cand in [\"genre_name\", \"name\", \"title\"]:\n",
    "            if cand in g.columns:\n",
    "                g = g.rename(columns={cand: \"genre\"})\n",
    "                break\n",
    "    g = g[[\"genre_id\", \"genre\"]]\n",
    "\n",
    "    bg = book_genres_df[[\"book_id\", \"genre_id\"]].copy()\n",
    "\n",
    "    meta = (\n",
    "        bg.merge(g, on=\"genre_id\", how=\"left\")\n",
    "          .merge(books_df, on=\"book_id\", how=\"left\")\n",
    "    )\n",
    "    return meta\n",
    "\n",
    "\n",
    "books_meta = build_books_metadata(books, genres, book_genres)\n",
    "\n",
    "\n",
    "def add_genre_features(candidates: pd.DataFrame,\n",
    "                       hist_df: pd.DataFrame,\n",
    "                       books_meta_df=None) -> pd.DataFrame:\n",
    "    df = candidates.copy()\n",
    "\n",
    "    if books_meta_df is None or \"genre\" not in books_meta_df.columns:\n",
    "        df[\"user_genre_share\"] = 0.0\n",
    "        df[\"genre_match\"] = 0\n",
    "        return df\n",
    "\n",
    "    df = df.merge(books_meta_df[[\"book_id\", \"genre\"]].drop_duplicates(),\n",
    "                  on=\"book_id\", how=\"left\")\n",
    "\n",
    "    tmp = (\n",
    "        hist_df[[\"user_id\", \"book_id\"]]\n",
    "        .merge(books_meta_df[[\"book_id\", \"genre\"]].drop_duplicates(),\n",
    "               on=\"book_id\", how=\"left\")\n",
    "        .dropna(subset=[\"genre\"])\n",
    "    )\n",
    "\n",
    "    user_genre_counts = (\n",
    "        tmp.groupby([\"user_id\", \"genre\"])[\"book_id\"]\n",
    "           .size()\n",
    "           .reset_index(name=\"cnt\")\n",
    "    )\n",
    "\n",
    "    total = user_genre_counts.groupby(\"user_id\")[\"cnt\"].transform(\"sum\")\n",
    "    user_genre_counts[\"genre_share\"] = user_genre_counts[\"cnt\"] / total\n",
    "\n",
    "    idx = user_genre_counts.groupby(\"user_id\")[\"cnt\"].idxmax()\n",
    "    fav_genre = (\n",
    "        user_genre_counts.loc[idx, [\"user_id\", \"genre\"]]\n",
    "        .rename(columns={\"genre\": \"preferred_genre\"})\n",
    "    )\n",
    "\n",
    "    df = df.merge(\n",
    "        user_genre_counts[[\"user_id\", \"genre\", \"genre_share\"]],\n",
    "        on=[\"user_id\", \"genre\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df[\"user_genre_share\"] = df[\"genre_share\"].fillna(0).astype(\"float32\")\n",
    "    df = df.drop(columns=[\"genre_share\"], errors=\"ignore\")\n",
    "\n",
    "    df = df.merge(fav_genre, on=\"user_id\", how=\"left\")\n",
    "    df[\"genre_match\"] = (df[\"genre\"] == df[\"preferred_genre\"]).astype(\"int8\")\n",
    "    df[\"genre_match\"] = df[\"genre_match\"].fillna(0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "485110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_basic_features(candidates: pd.DataFrame,\n",
    "                       user_stats: pd.DataFrame,\n",
    "                       book_stats: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = candidates.copy()\n",
    "    df = df.merge(user_stats, on=\"user_id\", how=\"left\")\n",
    "    df = df.merge(book_stats, on=\"book_id\", how=\"left\")\n",
    "    return df.fillna(0)\n",
    "\n",
    "\n",
    "def add_all_features(candidates: pd.DataFrame,\n",
    "                     hist_df: pd.DataFrame,\n",
    "                     user_stats: pd.DataFrame,\n",
    "                     book_stats: pd.DataFrame,\n",
    "                     split_time,\n",
    "                     books_meta_df=None) -> pd.DataFrame:\n",
    "    df = add_basic_features(candidates, user_stats, book_stats)\n",
    "    df = add_interaction_features(df, hist_df)\n",
    "    df = add_temporal_features(df, hist_df, split_time=split_time)\n",
    "    df = add_popularity_trend_features(df, hist_df)\n",
    "    df = add_genre_features(df, hist_df, books_meta_df)\n",
    "    return df.fillna(0)\n",
    "\n",
    "\n",
    "def convert_datetime_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if is_datetime64_any_dtype(df[c]):\n",
    "            df[c] = df[c].view(\"int64\") // 10**9\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40737b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_pos (53812, 3)\n",
      "val_cold (60495, 3)\n",
      "val_candidates (113171, 3)\n"
     ]
    }
   ],
   "source": [
    "val_period = val_period.copy()\n",
    "val_period[\"rel\"] = np.where(val_period[\"has_read\"] == 1, 2, 1)\n",
    "val_pos = val_period[[\"user_id\", \"book_id\", \"rel\"]].drop_duplicates()\n",
    "print(\"val_pos\", val_pos.shape)\n",
    "\n",
    "user_hist_books_hist, popular_books_hist = build_history_and_popularity(train_hist)\n",
    "val_users = val_period[\"user_id\"].unique()\n",
    "val_cold = build_cold_candidates(val_users, user_hist_books_hist,\n",
    "                                popular_books_hist, n_cold=N_COLD)\n",
    "print(\"val_cold\", val_cold.shape)\n",
    "\n",
    "val_candidates = pd.concat([val_pos, val_cold], ignore_index=True)\n",
    "val_candidates = val_candidates.drop_duplicates([\"user_id\", \"book_id\"])\n",
    "print(\"val_candidates\", val_candidates.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf412f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (305519, 3)\n",
      "Удаляю datetime-фичи: ['u_read_ts_mean', 'u_read_ts_max', 'u_read_ts_min', 'b_ts_mean', 'b_ts_max', 'b_ts_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/bvy2r4yn2xj5q3847gzk4r0m0000gn/T/ipykernel_26680/3289221636.py:57: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  logs[\"ts_days\"] = logs[\"timestamp\"].view(\"int64\") // (24 * 3600 * 10**9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаляю datetime-фичи: ['u_read_ts_mean', 'u_read_ts_max', 'u_read_ts_min', 'b_ts_mean', 'b_ts_max', 'b_ts_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/bvy2r4yn2xj5q3847gzk4r0m0000gn/T/ipykernel_26680/3289221636.py:57: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  logs[\"ts_days\"] = logs[\"timestamp\"].view(\"int64\") // (24 * 3600 * 10**9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features 22\n",
      "cat features ['genre', 'preferred_genre']\n",
      "xgb feature 20\n",
      "Распределение rel (train)\n",
      "rel\n",
      "0    242819\n",
      "1    198545\n",
      "2    289893\n",
      "Name: count, dtype: int64\n",
      "Распределение rel (val)\n",
      "rel\n",
      "0    158821\n",
      "1     51859\n",
      "2     69355\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_pos = train_hist.copy()\n",
    "train_pos[\"rel\"] = np.where(train_pos[\"has_read\"] == 1, 2, 1)\n",
    "train_pos = train_pos[[\"user_id\", \"book_id\", \"rel\"]].drop_duplicates()\n",
    "\n",
    "train_users = train_hist[\"user_id\"].unique()\n",
    "user_hist_books_train, popular_books_train = build_history_and_popularity(train_hist)\n",
    "train_cold = build_cold_candidates(train_users, user_hist_books_train, popular_books_train, n_cold=N_COLD)\n",
    "\n",
    "train_candidates_hist = pd.concat([train_pos, train_cold], ignore_index=True)\n",
    "train_candidates_hist = train_candidates_hist.drop_duplicates([\"user_id\", \"book_id\"])\n",
    "print(\"train\", train_candidates_hist.shape)\n",
    "\n",
    "train_features_hist = add_all_features(\n",
    "    train_candidates_hist, train_hist,\n",
    "    user_stats_hist, book_stats_hist,\n",
    "    split_time=split_point,\n",
    "    books_meta_df=books_meta,\n",
    ")\n",
    "val_features = add_all_features(\n",
    "    val_candidates, train_hist,\n",
    "    user_stats_hist, book_stats_hist,\n",
    "    split_time=split_point,\n",
    "    books_meta_df=books_meta,\n",
    ")\n",
    "\n",
    "train_features_hist = convert_datetime_to_numeric(train_features_hist)\n",
    "val_features = convert_datetime_to_numeric(val_features)\n",
    "\n",
    "feature_cols = [c for c in train_features_hist.columns if c not in [\"user_id\", \"book_id\", \"rel\"]]\n",
    "\n",
    "cat_features = [c for c in [\"genre\", \"preferred_genre\"] if c in feature_cols]\n",
    "\n",
    "xgb_feature_cols = [c for c in feature_cols if train_features_hist[c].dtype != \"O\"]\n",
    "\n",
    "print(\"num features\", len(feature_cols))\n",
    "print(\"cat features\", cat_features)\n",
    "print(\"xgb feature\", len(xgb_feature_cols))\n",
    "\n",
    "X_train_full = train_features_hist[feature_cols]\n",
    "X_val_full = val_features[feature_cols]\n",
    "\n",
    "X_train_xgb = train_features_hist[xgb_feature_cols]\n",
    "X_val_xgb = val_features[xgb_feature_cols]\n",
    "\n",
    "y_train_rel  = train_features_hist[\"rel\"]\n",
    "y_train_read = (y_train_rel == 2).astype(int)\n",
    "y_train_any  = (y_train_rel > 0).astype(int)\n",
    "\n",
    "y_val_rel = val_features[\"rel\"]\n",
    "y_val_read = (y_val_rel == 2).astype(int)\n",
    "y_val_any = (y_val_rel > 0).astype(int)\n",
    "\n",
    "print(\"Распределение rel (train)\")\n",
    "print(y_train_rel.value_counts().sort_index())\n",
    "print(\"Распределение rel (val)\")\n",
    "print(y_val_rel.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dcdbb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  book_id  rel      pred\n",
      "0  1551451  2573361    2  0.291631\n",
      "1  1551451  2573361    2  0.264879\n",
      "2  1397150  2538344    2  0.075261\n",
      "3  1397150  2538344    2  0.075234\n",
      "4  1358090  2019613    2  0.000487\n",
      "NDCG@20: 0.936428\n"
     ]
    }
   ],
   "source": [
    "xgb_read = xgb.XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_any = xgb.XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=43,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_read.fit(X_train_xgb.values, y_train_read.values)\n",
    "xgb_any.fit(X_train_xgb.values, y_train_any.values)\n",
    "\n",
    "p_read_xgb_val = xgb_read.predict_proba(X_val_xgb.values)[:, 1]\n",
    "p_any_xgb_val  = xgb_any.predict_proba(X_val_xgb.values)[:, 1]\n",
    "score_xgb_val  = W_READ * p_read_xgb_val + W_ANY * p_any_xgb_val\n",
    "\n",
    "cat_idx = [X_train_full.columns.get_loc(c) for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "cb_read = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=600,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=42,\n",
    "    l2_leaf_reg=3.0,\n",
    "    verbose=False\n",
    ")\n",
    "cb_any = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=600,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=43,\n",
    "    l2_leaf_reg=3.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_pool_read = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_read,\n",
    "    cat_features=cat_idx\n",
    ")\n",
    "train_pool_any = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_any,\n",
    "    cat_features=cat_idx\n",
    ")\n",
    "\n",
    "cb_read.fit(train_pool_read)\n",
    "cb_any.fit(train_pool_any)\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_val_full,\n",
    "    cat_features=cat_idx\n",
    ")\n",
    "\n",
    "p_read_cb_val = cb_read.predict_proba(val_pool)[:, 1]\n",
    "p_any_cb_val  = cb_any.predict_proba(val_pool)[:, 1]\n",
    "score_cb_val  = W_READ * p_read_cb_val + W_ANY * p_any_cb_val\n",
    "\n",
    "val_features[\"score_xgb\"] = score_xgb_val\n",
    "val_features[\"score_cb\"]  = score_cb_val\n",
    "val_features[\"pred\"] = W_XGB * score_xgb_val + W_CB * score_cb_val\n",
    "\n",
    "print(val_features[[\"user_id\", \"book_id\", \"rel\", \"pred\"]].head())\n",
    "\n",
    "ndcg20 = mean_ndcg(val_features, k=20)\n",
    "print(f\"NDCG@20: {ndcg20:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "616cf76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates (81048, 2)\n",
      "train full pairs (378396, 3)\n",
      "Удаляю datetime-фичи: ['u_read_ts_mean', 'u_read_ts_max', 'u_read_ts_min', 'b_ts_mean', 'b_ts_max', 'b_ts_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/bvy2r4yn2xj5q3847gzk4r0m0000gn/T/ipykernel_26680/3289221636.py:57: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  logs[\"ts_days\"] = logs[\"timestamp\"].view(\"int64\") // (24 * 3600 * 10**9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full rel distribution\n",
      "rel\n",
      "0    302883\n",
      "1    250404\n",
      "2    359248\n",
      "Name: count, dtype: int64\n",
      "Удаляю datetime-фичи: ['u_read_ts_mean', 'u_read_ts_max', 'u_read_ts_min', 'b_ts_mean', 'b_ts_max', 'b_ts_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/bvy2r4yn2xj5q3847gzk4r0m0000gn/T/ipykernel_26680/3289221636.py:57: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  logs[\"ts_days\"] = logs[\"timestamp\"].view(\"int64\") // (24 * 3600 * 10**9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  book_id      pred\n",
      "0      210    11936  0.000011\n",
      "1      210    11936  0.000011\n",
      "2      210    11936  0.000010\n",
      "3      210   254097  0.000032\n",
      "4      210   254097  0.000024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_stats_full, book_stats_full = compute_user_book_stats(train)\n",
    "\n",
    "cand = candidates_raw.copy()\n",
    "if \"book_id_list\" in cand.columns:\n",
    "    cand[\"book_id_list\"] = cand[\"book_id_list\"].fillna(\"\").astype(str)\n",
    "    cand[\"book_id_list\"] = cand[\"book_id_list\"].str.split(\",\")\n",
    "    cand_long = cand.explode(\"book_id_list\")\n",
    "    cand_long = cand_long[cand_long[\"book_id_list\"].str.strip() != \"\"]\n",
    "    cand_long[\"book_id\"] = cand_long[\"book_id_list\"].astype(int)\n",
    "    candidates_long = cand_long[[\"user_id\", \"book_id\"]].drop_duplicates()\n",
    "else:\n",
    "    candidates_long = cand[[\"user_id\", \"book_id\"]].drop_duplicates()\n",
    "\n",
    "print(\"candidates\", candidates_long.shape)\n",
    "\n",
    "train_pos_full = train.copy()\n",
    "train_pos_full[\"rel\"] = np.where(train_pos_full[\"has_read\"] == 1, 2, 1)\n",
    "train_pos_full = train_pos_full[[\"user_id\", \"book_id\", \"rel\"]].drop_duplicates()\n",
    "\n",
    "train_users_full = train[\"user_id\"].unique()\n",
    "user_hist_books_full, popular_books_full = build_history_and_popularity(train)\n",
    "train_cold_full = build_cold_candidates(train_users_full, user_hist_books_full, popular_books_full, n_cold=N_COLD)\n",
    "\n",
    "train_full_pairs = pd.concat([train_pos_full, train_cold_full], ignore_index=True)\n",
    "train_full_pairs = train_full_pairs.drop_duplicates([\"user_id\", \"book_id\"])\n",
    "print(\"train full pairs\", train_full_pairs.shape)\n",
    "\n",
    "train_features_full = add_all_features(\n",
    "    train_full_pairs, train,\n",
    "    user_stats_full, book_stats_full,\n",
    "    split_time=split_point,\n",
    "    books_meta_df=books_meta,\n",
    ")\n",
    "train_features_full = convert_datetime_to_numeric(train_features_full)\n",
    "\n",
    "common_features = [c for c in feature_cols if c in train_features_full.columns]\n",
    "xgb_common = [c for c in xgb_feature_cols if c in common_features]\n",
    "\n",
    "X_full_all = train_features_full[common_features]\n",
    "X_full_xgb = train_features_full[xgb_common]\n",
    "\n",
    "y_full_rel  = train_features_full[\"rel\"]\n",
    "y_full_read = (y_full_rel == 2).astype(int)\n",
    "y_full_any  = (y_full_rel > 0).astype(int)\n",
    "\n",
    "print(\"full rel distribution\")\n",
    "print(y_full_rel.value_counts().sort_index())\n",
    "\n",
    "xgb_read_full = xgb.XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=52,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgb_any_full = xgb.XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=53,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_read_full.fit(X_full_xgb.values, y_full_read.values)\n",
    "xgb_any_full.fit(X_full_xgb.values, y_full_any.values)\n",
    "\n",
    "cat_idx_full = [X_full_all.columns.get_loc(c) for c in cat_features if c in X_full_all.columns]\n",
    "\n",
    "cb_read_full = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=600,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=52,\n",
    "    l2_leaf_reg=3.0,\n",
    "    verbose=False\n",
    ")\n",
    "cb_any_full = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=600,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=53,\n",
    "    l2_leaf_reg=3.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_pool_read_full = Pool(\n",
    "    data=X_full_all,\n",
    "    label=y_full_read,\n",
    "    cat_features=cat_idx_full\n",
    ")\n",
    "train_pool_any_full = Pool(\n",
    "    data=X_full_all,\n",
    "    label=y_full_any,\n",
    "    cat_features=cat_idx_full\n",
    ")\n",
    "\n",
    "cb_read_full.fit(train_pool_read_full)\n",
    "cb_any_full.fit(train_pool_any_full)\n",
    "\n",
    "test_features = add_all_features(\n",
    "    candidates_long, train,\n",
    "    user_stats_full, book_stats_full,\n",
    "    split_time=split_point,\n",
    "    books_meta_df=books_meta,\n",
    ")\n",
    "test_features = convert_datetime_to_numeric(test_features)\n",
    "\n",
    "test_features = test_features.merge(\n",
    "    train_features_full[[\"user_id\", \"book_id\"] + common_features],\n",
    "    on=[\"user_id\", \"book_id\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_train_full\")\n",
    ")\n",
    "\n",
    "for c in common_features:\n",
    "    if c not in test_features.columns:\n",
    "        test_features[c] = 0.0\n",
    "\n",
    "X_test_all = test_features[common_features]\n",
    "X_test_xgb = test_features[xgb_common]\n",
    "\n",
    "p_read_xgb_test = xgb_read_full.predict_proba(X_test_xgb.values)[:, 1]\n",
    "p_any_xgb_test  = xgb_any_full.predict_proba(X_test_xgb.values)[:, 1]\n",
    "score_xgb_test  = W_READ * p_read_xgb_test + W_ANY * p_any_xgb_test\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_test_all,\n",
    "    cat_features=[X_test_all.columns.get_loc(c) for c in cat_features if c in X_test_all.columns]\n",
    ")\n",
    "p_read_cb_test = cb_read_full.predict_proba(test_pool)[:, 1]\n",
    "p_any_cb_test  = cb_any_full.predict_proba(test_pool)[:, 1]\n",
    "score_cb_test  = W_READ * p_read_cb_test + W_ANY * p_any_cb_test\n",
    "\n",
    "test_features[\"pred\"] = W_XGB * score_xgb_test + W_CB * score_cb_test\n",
    "\n",
    "print(test_features[[\"user_id\", \"book_id\", \"pred\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c6d0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                                       book_id_list\n",
      "0      210  1673950,971259,1281035,3015694,2447113,2225251...\n",
      "1     1380  2548861,2290484,482934,1326209,2379664,1098150...\n",
      "2     2050  1021078,460492,317849,2053462,867246,2254200,2...\n",
      "3     2740  987516,1296620,2327258,1834192,2307893,549194,...\n",
      "4     4621  2595660,1809950,1964216,2446687,2347566,244668...\n",
      "Саб сохранён в /Users/steksov_grigoriy/Desktop/НТО/individ/public/submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "submission_user_list = make_submission_user_list(test_features, top_k=20)\n",
    "print(submission_user_list.head())\n",
    "\n",
    "SUBMIT_PATH = os.path.join(DATA_DIR, \"submission.csv\")\n",
    "submission_user_list.to_csv(SUBMIT_PATH, index=False)\n",
    "print(\"Саб сохранён в\", SUBMIT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nto_comand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
